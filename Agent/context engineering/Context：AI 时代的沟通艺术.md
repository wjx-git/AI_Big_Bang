# Context Engineering：AI 时代的沟通艺术

Prompt Engineering（提示工程）作为一门艺术和科学，教会我们如何通过精心设计的指令，引导AI生成高质量的输出。然而，随着AI应用从简单的问答走向更复杂的任务，Prompt的局限性也日益显现。

## 1. Prompt 的局限性

Prompt，即用户提供给AI模型的输入指令，通常包括任务、指令、角色等要素。它在单轮交互中表现出色，但当任务变得复杂、需要多轮对话或依赖外部信息时，仅凭Prompt往往难以满足需求。例如，当用户向大模型发送提示：“给Jim发邮件，找个时间见面”，模型只能生成一封泛泛而谈的草稿：`嗨Jim！明天上午见个面，已经发了邀请，告诉我是否方便。`。这是因为模型缺乏必要的“上下文”信息，无法理解用户的真实意图和个性化需求。

这种局限性体现在：

*   **信息不足**：Prompt本身无法承载用户的所有背景信息、偏好、历史数据等，导致AI输出缺乏个性化和精准度。
*   **无状态性**：传统的Prompt交互是无状态的，每次交互都是独立的，AI无法记住之前的对话或任务进展，难以处理连续性任务。
*   **缺乏外部知识**：AI模型仅基于其训练数据进行响应，无法实时获取外部工具、检索信息或用户私有数据，限制了其解决实际问题的能力。

这些缺陷使得AI模型在面对复杂、动态的任务时，即便拥有强大的推理能力，也可能因为“世界观”的不完整而表现不佳。**这正是Context Engineering应运而生的原因**。



## 2. 什么是 Context Engineering

随着大模型（LLMs）从基本的指令遵循系统发展为复杂应用的核心推理引擎，设计和管理其信息负载的方法也随之演进，逐渐形成了一门正式的学科——**上下文工程（Context Engineering）**。

**定义**：上下文工程是构建动态系统，以正确的格式在恰当的时机提供恰当的信息和工具给大模型，从而使大模型能够合理地完成任务。它标志着从无状态、单轮交互的提示工程时代，向有状态、多轮交互的复杂人工智能系统的根本性演进。如果说prompt工程侧重于设计完美的指令，那么上下文工程则致力于构建模型正确理解该指令并有效采取行动所需的全部知识世界。

Langchain 在其博客《上下文工程的兴起》（The Rise of Context Engineering）中提到：

> 当大模型表现不佳时，通常是因为缺乏适当的上下文。随着大模型应用逐渐演变为更复杂的系统，上下文工程正成为AI工程师的一项关键技能。其目标是为大模型提供必要的上下文，以确保其可靠地执行任务。

### Context 与 Prompt 的区别

为了更好地理解上下文工程，我们需要明确它与Prompt的区别：

*   **Prompt**：用户使用AI应用时所提供的输入，例如即时任务或问题，可以包括示例、规则等。它是用户通过AI应用提供给大模型的输入。
*   **上下文 (Context)**：AI应用系统自行收集和用户任务相关的信息，例如用户的输入、系统指令、示例、规则、对话历史、长期记忆、工具、检索的信息等，将这些信息整理成上下文提供给大模型作为输入。简单理解就是模型在输出前所看到的内容就是上下文。

因此，**上下文可理解为大模型推理时的全部输入内容，而Prompt只是上下文中的一部分内容。** 上下文工程代表了AI领域的一次范式转变，将大模型的使用从简单的“提示词输入”提升为复杂的“上下文协调与编排”。

### Context Engineering 的优势：以 Email 示例说明

AI 应用开发的前沿已从以模型为中心的优化，转向以上下文为中心的架构设计。最强大的模型之所以表现不佳，并非因其内在缺陷，而是因为它们所获得的世界视图是不完整、"半成品式"的。上下文工程正是为了解决这一问题，通过构建丰富的“环境”来释放模型的潜力。

考虑前文的例子：给Jim发邮件，找个时间见面。一个基于上下文工程原则构建的系统会首先生成一个“上下文快照”（contextual snapshot）。这个快照包括：

*   用户的日程安排和空闲时间；
*   用户惯常的会议偏好（例如喜欢上午、30分钟时长）；
*   与“Jim”过往的交流历史；
*   用户偏好的语气风格（“简洁、果断、亲切”）。

将这些丰富的上下文信息提供给同一个大模型后，系统便能生成一条“神奇”且立即可用的回复，例如：`嗨Jim！我明天全天排满了，一直有会。周四上午我有空，你看行吗？已经发了邀请，告诉我是否方便。`。 模型本身并没有变得更“聪明”，而是它的环境变得更丰富了。这正体现了核心原则：**价值的释放不在于更换模型，而在于优化上下文。**

通过上下文工程，我们不再仅仅关注如何向AI提问，而是如何为AI构建一个“智慧”的环境，让它能够获取并利用所有必要的信息和工具，从而更准确、更高效地完成任务。这使得AI应用能够从单次、孤立的交互，演变为持续、智能的协作伙伴。


## 3. Context Engineering 的重要性

尽管前沿模型的上下文窗口不断扩展，支持多达100万个 token，但这并不意味着更长的上下文窗口必然带来更好的响应。事实上，过度加载上下文可能导致AI Agent和应用程序以意想不到的方式失败。上下文可能会被污染、分散注意力、令人困惑或相互冲突。这对于依赖上下文来收集信息、综合发现和协调行动的AI Agent来说尤其成问题。

主要的挑战包括：

1.  **上下文污染（Context Poisoning）**：指当幻觉或其他错误信息进入LLM的上下文并被反复引用时。例如，一个Agent在执行任务时产生错误信息，这些信息被模型“记住”并持续影响后续的判断和决策，导致Agent固执于不可能或不相关的目标。
2.  **上下文干扰（Context Distraction）**：发生在上下文过长，导致LLM过度关注上下文中的冗余信息，而忽略了其在训练中学到的核心知识。这使得Agent倾向于重复历史动作而非生成新的、更优的计划。例如，在Agent工作流中，当上下文增长到超过10万token后，Agent倾向于重复其庞大历史中的动作，而不是综合出新的计划。
3.  **上下文困惑（Context Confusion）**：当上下文中的冗余内容被LLM用于生成低质量响应时。即使提供了大量工具描述，模型也可能因信息过载而性能下降，甚至尝试使用不相关的工具。LLM会关注上下文中的所有内容，即使是无关信息，这会降低响应的准确性。例如，当一个模型被提供了46种工具的描述，即使上下文窗口足够，它也可能因为信息过载而无法有效选择和使用工具。
4.  **上下文冲突（Context Clash）**：指上下文中新获取的信息或工具与现有信息发生直接冲突。例如，在多轮对话中分阶段提供信息，可能导致模型在早期轮次做出错误假设并过分依赖这些错误答案，从而显著降低最终性能。这种内部矛盾会使Agent的推理过程脱轨。

这些失败模式对AI Agent的影响尤为显著，因为Agent通常在需要收集多源信息、进行顺序工具调用、多轮推理和积累大量历史的复杂场景中运行，这些场景极易导致上下文膨胀，从而放大上述问题。

**大多数Agent的失败，已不再是模型失败，而是上下文失败**。上下文工程在提升AI系统性能方面扮演着至关重要的角色：

1.  **提升推理能力**：通过提供最相关、最简洁的信息，AI Agent能够更准确地理解任务、进行有效推理并生成高质量的响应。这使得模型能够超越其预训练知识，利用实时、特定领域的信息进行决策。
2.  **提高运行效率**：优化上下文的使用可以减少不必要的计算和信息处理，从而加速Agent的响应时间，并降低资源消耗。精准的上下文能够引导模型直达问题核心，避免无效的探索。
3.  **增强Agent鲁棒性**：有效的上下文管理有助于避免信息过载或错误信息引起的故障，使Agent在复杂多变的环境中保持稳定表现。通过过滤噪声和识别冲突，上下文工程能够提高系统的容错能力。
4.  **实现复杂任务**：对于需要长期记忆、多步骤规划或与外部工具交互的复杂任务，上下文工程是确保Agent能够有效执行这些任务的基础。它为Agent提供了执行复杂操作所需的“世界模型”和“行动指南”。

## 4. 管理的艺术

上下文管理不仅仅是技术问题，更是架构设计的艺术。一个设计精良的上下文管理系统能够让AI Agent在有限的计算资源下，展现出超越其基础模型能力的智能表现。相反，粗糙的上下文处理往往导致信息丢失、响应不一致，甚至完全偏离用户意图。

现代前沿LLM虽然已支持128K乃至更大的上下文窗口，但在真实的智能体场景中，这往往仍不够用，有时甚至反而成为负担。观测结果可能非常庞大，尤其是当智能体与网页或PDF这类非结构化数据交互时，很容易就会超出上下文限制。同时，模型性能在超过某个上下文长度后往往会下降，即使窗口技术层面仍支持。此外，长输入成本高昂，即使有前缀缓存，仍需为传输和预填充每个token付费。

本节将深入探讨上下文管理的四个核心维度：保存（Save）、选择（Select）、压缩（Compress）和隔离（Isolate），为构建高效、智能的AI系统提供系统性的指导。


### 保存Context：构建AI的记忆体系

要构建真正智能、连贯的多轮对话或自主Agent系统，仅靠模型自身远远不够，必须引入外部记忆机制。这正是保存context的目的：将状态和学到的知识持久化，以便未来读取与推理。我们可以将智能体的记忆系统划分为三个层次：**短期记忆、长期记忆**和**草稿本（Scratchpad）**，分别对应不同时间尺度和用途的信息管理。

#### 短期记忆：当前会话的上下文管理

短期记忆聚焦于当前会话的上下文管理，即对话历史的维护。其核心目标是在保持语义连贯性的同时，避免超出模型上下文窗口的限制。这一层次的记忆管理直接影响着用户体验的连贯性和系统响应的准确性。

在实际应用中，短期记忆的管理策略多种多样。最简单的方法是"滑动窗口"策略，即仅保留最近N轮对话，当新的交互产生时，自动丢弃最早的对话记录。这种方法实现简单，但可能导致重要信息的丢失，特别是在长时间对话中，早期建立的上下文可能对后续交互仍然重要。

更智能的方法包括使用小型语言模型定期总结历史对话。这种方法能够保留对话的核心信息，同时大幅减少token消耗。总结过程可以在特定触发条件下进行，比如当上下文长度达到阈值时，或者在对话主题发生明显转换时。

另一种先进的方法是基于嵌入（embedding）的检索机制。系统将历史对话转换为向量表示，并根据当前查询的语义相似性，动态提取最相关的过往消息。这种方法实现了上下文的高效压缩与召回，能够在保持相关性的同时，显著减少上下文窗口的占用。

#### 长期记忆：跨会话的知识积累

长期记忆着眼于跨会话的持久化知识积累，是构建个性化AI体验的关键。当用户表达"我是素食者"或"我喜欢极简风格的设计"这样的偏好时，这些关键信息需要被提取并以向量形式存储在专用数据库中。后续交互中，系统可通过语义检索唤醒这些记忆，实现个性化响应。

长期记忆的实现通常依赖于向量数据库技术。用户的偏好、行为模式、专业背景等信息被编码为高维向量，并建立索引以支持快速检索。当新的交互发生时，系统会根据当前上下文查询相关的长期记忆，并将检索到的信息融入到当前的推理过程中。

这种机制让LLM具备了"记住用户"的能力，是构建可成长、可进化的智能体的关键。随着交互次数的增加，系统对用户的了解越来越深入，能够提供越来越精准和个性化的服务。这不仅提升了用户体验，也为AI系统的商业价值创造了重要基础。

长期记忆的管理还涉及到记忆的更新和遗忘机制。并非所有信息都应该永久保存，系统需要能够识别过时或不再相关的信息，并适时进行清理。同时，当用户的偏好发生变化时，系统也需要能够更新相应的记忆内容。

#### 草稿本：任务执行的临时工作区

草稿本（Scratchpad）是智能体在单次复杂任务执行过程中的"临时笔记"，类似于人类解题时的草稿纸。它用于记录中间步骤、计划、观察结果或临时数据，对支持多步骤、长链条的推理至关重要。

在实际应用中，草稿本的使用场景非常广泛。例如，一个海报生成Agent可能将生成的图片链接暂存于运行时字典中，供后续步骤引用；数学推理Agent则可在此保存中间计算结果；代码生成Agent可能在草稿本中记录函数定义、变量状态等信息。多Agent系统可以保存规划步骤、中间思考过程或者推理步骤保存。

草稿本强调**临时性**与**任务内共享**的特点。它通常不跨会话保留，但在单个任务的执行过程中，不同的处理步骤可以共享和修改草稿本中的信息。这种设计既保证了信息的及时性，又避免了不必要的存储开销。

在Manus系统中，文件系统被视为终极上下文：容量无限、天然持久，并且Agent可直接操作。模型学会按需读写文件——把文件系统不仅当作存储，更当作结构化、外化的记忆。这种设计理念将草稿本的概念扩展到了整个文件系统层面，为复杂任务的执行提供了强大的支持。

#### 记忆系统的协同机制

三种记忆类型并非孤立存在，而是需要精心设计的协同机制。短期记忆为当前对话提供即时上下文，长期记忆提供个性化背景，草稿本支持复杂推理过程。它们之间的信息流动和转换规则，直接决定了整个系统的智能水平。

未来，智能体的真正竞争力不只在于模型的推理能力，更在于其记忆系统的完备性与灵活性。通过精心设计短期、长期与工作记忆的协同机制，我们正在一步步赋予LLM类人的记忆与学习能力，迈向真正可持续、自适应的人工智能。



### 选择Context：精炼AI的感知

选择上下文的核心目的是**将最相关、最必要的信息拉入有限的上下文窗口，以帮助Agent高效、准确地执行当前任务**。这就像一个高效的图书馆管理员，在浩瀚的知识海洋中，精准地为读者挑选出当下最需要的书籍。上下文的来源主要有四种：草稿本、记忆、工具和外部资料。

#### 从草稿本中选择

草稿本存储的是临时信息，通常这些信息会有标记类型或来源。在选择时，Agent会根据这些标记确定是否需要将草稿本中的内容纳入当前上下文。如果草稿本被设计为一个工具，Agent可以通过调用该工具来读取内容；如果是Agent运行时状态的一部分，则开发者可以在每一步控制哪些信息暴露给大语言模型（LLM），从而实现对上下文使用的精细控制。这种精细化控制对于确保Agent专注于当前任务，避免不必要的干扰至关重要。

#### 从记忆中选择

记忆是Agent积累的历史对话、任务相关的指令、示例和事实信息。选择历史对话的方法常采用检索增强生成（RAG）形式，即根据当前查询，从记忆库中检索出最相关的历史片段。对于指令、规则、示例等，则通常存储在预定义文件中，Agent根据当前任务动态选择。然而，记忆的选择并不总是准确，有时会导致不期望的结果。例如，在某些情况下，ChatGPT会意外地将用户的位置信息插入到生成的图像中，这种“上下文入侵”会让用户感到失控。这表明，在记忆检索过程中，需要更智能的过滤机制来确保只引入真正相关且无害的信息。

#### 从工具中选择

工具是Agent执行任务的重要能力扩展。然而，当Agent可访问的工具数量过多，或工具描述存在重叠时，容易造成模型混淆，难以选择正确的工具。例如，DeepSeek-v3团队发现，当工具数量超过30个时，工具描述开始重叠，导致混淆；超过100个工具时，模型几乎必然失败。一种有效的解决方案是采用基于RAG的技术，对工具的描述（名称、功能、参数等）进行检索。根据当前任务查询，动态检索最相关的一小部分工具及其描述送入上下文窗口，而不是一次性提供所有工具。这已被证明能显著提高工具选择的准确性，甚至在某些情况下能将工具选择准确率提高3倍。

#### 从外部资料中选择

对于外部资料，如文档库、代码库、数据库等，Agent同样需要根据用户查询或当前任务，从中检索出最相关的文档片段或信息块。这通常也依赖于RAG技术，是上下文工程的核心挑战之一，也在实际应用中展现了巨大价值。例如，代码Agent广泛利用RAG来检索和整合项目结构、函数定义等关键信息，从而更高效地完成编码任务。

综上所述，无论是草稿本、记忆、工具还是外部知识，它们的有效选择和整合对于Agent的表现至关重要。随着技术的发展，如何在保证上下文准确性和可控性的前提下，最大化Agent的能力，将成为持续探索的方向。



### 压缩Context：优化AI的思考效率

在构建和运行基于大语言模型的Agent时，随着交互轮次的增加以及工具调用的复杂化，上下文（context）往往会变得非常庞大，占用大量计算资源。为了解决这一问题，压缩上下文成为一种关键策略，其核心目标是在不丢失关键信息的前提下，保留完成任务所需的最小有效信息。

百万级上下文窗口的出现，并没有消除上下文工程的需求；相反，它使这一需求变得更加迫切。更大的上下文窗口并不等于更聪明的大脑，而更像是一个更大、更嘈杂的房间，需要一位更精明的图书管理员来管理。研究持续表明，大语言模型存在“中间信息丢失”问题：它们对上下文开头和结尾的信息回忆能力最强，而对埋藏在中间的信息回忆能力则显著下降。此外，处理长上下文在计算上开销巨大且速度缓慢。这使得简单粗暴的上下文堆砌变得无效。最先进的系统已经认识到，架构设计比原始上下文长度更为重要；一个设计精良、上下文经过精心筛选的检索增强生成（RAG）系统，其表现可以超过一个依赖百万级上下文窗口但设计粗糙的系统。因此，“压缩”（Compress）模式对于有效管理大规模上下文至关重要。

以下是几种关键的上下文压缩技术：

#### 上下文总结（Context Summarization）

上下文总结是一种常见做法，是指利用大语言模型对长文档或对话历史生成简洁的摘要，再将摘要而非原文加入上下文。例如，在Claude Code中，当上下文接近最大容量时，系统会自动执行“auto-compact”机制，对整个用户与Agent之间的对话轨迹进行总结。这种总结可以采用递归或分层的方式进行，以适应更长、更复杂的交互历史。

总结过程面临的挑战是如何保留重要的决策点或事件信息。为此，一些系统甚至采用专门微调的模型来提升总结质量。对上下文进行摘要本身操作简单，但要为特定智能体实现完美的摘要却极具挑战。明确哪些信息应被保留，并将这些要求清晰传达给由大语言模型（LLM）驱动的压缩步骤，对智能体开发者而言至关重要。因此，值得将“摘要”这一功能独立出来，作为一个单独的LLM驱动的处理阶段或应用模块。这样做不仅便于管理，还能收集评估数据，直接用于优化和改进摘要过程。

#### 上下文裁剪（Context Trimming）

上下文裁剪不同于依赖模型进行语义提炼的总结方式，裁剪更多依靠规则或启发式策略来移除不相关或不重要的信息。例如，可以通过硬编码逻辑删除较早的历史消息，或者使用训练好的“上下文剪枝器”（如Provence模型，仅1.75 GB）来判断哪些内容可以安全去除。这种方法通常效率更高，但可能牺牲一定的上下文完整性。

#### 结构化提取（Structured Extraction）

结构化提取是指在multi-Agent系统中，不将原始的非结构化文本直接传给master Agent，而是先使用一个较小的LLM进行预处理，将关键信息提取为高度节省token的结构化格式（如JSON）。这种方法不仅节省了token开销，还使master Agent能更高效、更可靠地解析和使用这些信息。

总的来说，无论是总结、裁剪还是结构化提取，其本质都是在信息完整性和计算效率之间寻找平衡，帮助Agent在有限的上下文窗口中更好地完成复杂任务。

>Sentinel框架：高效的上下文压缩实践\
>Sentinel框架是一个典型的上下文压缩实践方法，它提供了一种轻量级但极为高效的方法，用于在将检索到的上下文传递给主语言模型（LLM）之前对其进行压缩。Sentinel的核心机制既巧妙又高效：

>1.  **问题重构**：Sentinel没有采用训练一个大型专用模型来执行压缩任务（这种方式成本高昂且缺乏可移植性），而是将压缩问题重新定义为一种基于注意力的“理解任务”。
>2.  **使用Agent模型**：它采用一个小型、现成的“Agent”语言模型（例如，一个参数量为5亿的小模型），将检索到的文档和用户查询输入该小模型。
>3.  **探测注意力机制**：Sentinel并不关心Agent模型生成的文本内容，而是探测其内部解码器的注意力分数。具体来说，它分析模型在生成最后一个输出token时，对各个输入句子的注意力分布。其核心假设（且在实验中被证实成立）是：与查询高度相关的句子，在模型准备生成答案时会获得更强的注意力。
>4.  **轻量级分类**：这些注意力信号被提取为每个句子的特征向量，并使用一个极其简单的轻量级分类器（逻辑回归模型）将这些注意力特征映射为相关性评分。
>5.  **过滤筛选**：在推理阶段，利用该Agent模型和分类器对每个句子进行相关性打分，仅保留得分最高的句子，并将这些精选后的上下文传递给大型、昂贵的生成式LLM。

>Sentinel的关键优势在于其高效性和可移植性。其核心发现是：查询与上下文之间的相关性信号在不同规模的模型之间表现出惊人的一致性。这意味着一个仅5亿参数的小模型，可以非常有效地Agent一个700亿参数的巨型模型，判断哪些上下文信息更为重要。在LongBench基准测试中，Sentinel能够实现高达5倍的上下文压缩，同时在问答性能上仍能媲美使用完整、未压缩上下文的系统，甚至优于许多更大、更复杂的压缩模型。






### 隔离Context：划分AI的职责边界

“隔离”（Isolate）模式通过严格划分上下文来解决AI系统中的信息混乱问题。不同的任务、子Agent或对话线程应在各自独立的上下文窗口中运行，这一过程通常由编排层（orchestration layer）进行管理。上下文隔离（Context Quarantine）是指将不同的上下文分别隔离在各自独立的线程中，每个线程由一个或多个大语言模型（LLM）单独使用。

在构建高效能的 mulit Agent系统时，上下文隔离是一个关键策略。通过将上下文进行拆分和管理，可以帮助Agent更专注地完成特定任务，从而提升整体表现。

#### 多Agent架构：分而治之

一种流行的方法是采用多Agent架构，例如OpenAI Swarm库所倡导的“关注点分离”理念，即通过一组各司其职的子Agent来处理不同的子任务。每个Agent拥有独立的工具集、指令和上下文窗口，这种分工方式在实践中表现出色，正如Anthropic的研究所示，多个具有隔离上下文的Agent在性能上优于单一Agent。

然而，多Agent架构也带来了挑战，比如更高的token消耗（据Anthropic报告最多可高出15倍），需要精心设计提示词来规划子Agent的工作流程，以及协调多个Agent之间的交互。这意味着在享受多Agent带来的性能提升的同时，也需要投入更多精力在系统设计和优化上。

#### 环境隔离：沙箱化执行

除了多Agent方式，另一种实现上下文隔离的方法是通过环境隔离。HuggingFace的研究提供了一个典型案例：他们使用一个Code Agent来生成工具调用指令，并在一个沙箱环境中运行这些代码。工具调用的结果中只选取必要的部分反馈给语言模型，从而避免不必要的上下文干扰。这种方法有效地防止了不相关信息对LLM的干扰，提高了Agent的执行效率和准确性。

#### 运行时状态对象：精细化控制

此外，Agent的运行时状态对象也可以作为上下文隔离的有效手段。通过定义结构化的状态对象，开发者可以有选择地将某些字段暴露给语言模型，而将其他信息隔离存储，仅在需要时调用。这种方式不仅实现了类似沙箱的效果，还能更好地控制信息流动，提升系统的可控性和效率。例如，LangGraph等框架围绕一个中心状态对象（central state object）进行设计。对于每个独立的工作流或会话，都会维护一个单独的状态对象。图结构的逻辑确保在任意给定步骤中，只有该状态的相关部分（例如当前子任务的指令、相关记忆等）才会被传入大语言模型的上下文，从而避免其他无关进程的干扰。

综上所述，上下文隔离不仅是优化Agent性能的重要手段，也为构建复杂、高效的AI系统提供了坚实的基础。无论是通过多Agent协作、环境隔离，还是状态对象的设计，核心目标都是让每一个模块专注于自己的职责，减少干扰，提高整体系统的智能水平与稳定性。



### 通往更智能AI的必由之路

大语言模型上下文管理是构建高效、智能AI Agent系统的核心挑战。本节从保存、选择、压缩和隔离四个维度，深入探讨了如何有效管理LLM的上下文。从短期记忆、长期记忆和草稿本的协同，到基于RAG的智能选择，再到上下文总结、裁剪和结构化提取的压缩技术，以及多Agent架构和环境隔离的隔离策略，每一步都旨在让AI系统在有限的资源下，实现更精准、更连贯、更智能的交互。

上下文管理不仅仅是技术细节，更是架构设计的艺术。一个设计精良的上下文管理系统，能够让AI Agent在复杂多变的环境中，保持清晰的“思维”，做出准确的判断。随着AI技术的不断演进，对上下文管理的深入理解和创新实践，将是通往更强大、更通用人工智能的必由之路。

# 5. 总结
本文深入探讨了从**提示工程**（Prompt Engineering）到**上下文工程**（Context Engineering）的范式转变，指出在AI应用日益复杂的背景下，仅靠优化提示词已不足以释放大模型的全部潜力。

文章首先指出**Prompt的局限性**：信息不足、无状态性、缺乏外部知识，导致AI在处理复杂任务时“世界观”不完整。为此，**上下文工程**应运而生——它通过系统性地构建、管理并适时提供任务所需的全部信息（包括用户偏好、历史、工具、记忆等），为AI打造一个“完整的环境”，从而提升其推理与执行能力。

博客进一步阐述了上下文工程的**四大核心管理策略**：
1.  **保存**（Save）：建立短期记忆、长期记忆和草稿本，构建AI的多层次记忆体系。
2.  **选择**（Select）：利用RAG等技术，精准筛选最相关的上下文，避免信息过载。
3.  **压缩**（Compress）：通过总结、裁剪、结构化提取等方式，在保留关键信息的同时优化效率，如Sentinel框架利用小模型注意力机制实现高效压缩。
4.  **隔离**（Isolate）：通过多Agent架构、沙箱环境或状态对象，划分职责边界，防止上下文污染与干扰。

总结而言，**上下文工程**已成为构建高性能AI系统的核心技能，其本质是从“如何提问”转向“如何为AI构建智慧环境”，通过科学管理上下文，让模型在复杂任务中更智能、高效、稳健地运行。

# 参考文献
[Context Engineering](https://blog.langchain.com/context-engineering-for-agents/)\
[How to Fix Your Context](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html)\
[Karpathy：我不是要造新词，是「上下文工程」对 Agent 来说太重要了](https://hub.baai.ac.cn/view/47077)\
[Context Engineering（上下文工程）是 AI Agent 成功的关键吗？](https://zhuanlan.zhihu.com/p/1925152531727758167)\
[The rise of "context engineering"](https://blog.langchain.com/the-rise-of-context-engineering/)\
[Context Engineering: Path towards better Agent Engineering](https://medium.com/superagentic-ai/context-engineering-path-towards-better-agent-engineering-412d7f9bf9f2)\
[Context engineering is what makes AI magical](https://boristane.com/blog/context-engineering/)\
[The New Skill in AI is Not Prompting, It's Context Engineering](https://www.philschmid.de/context-engineering)\
[How Long Contexts Fail](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html?ref=blog.langchain.com#context-clash)\
[Context Engineering: A Framework for Robust Generative AI Systems](https://www.sundeepteki.org/blog/context-engineering-a-framework-for-robust-generative-ai-systems)\
[Context Engineering for Agents](https://rlancemartin.github.io/2025/06/23/context_engineering/)


